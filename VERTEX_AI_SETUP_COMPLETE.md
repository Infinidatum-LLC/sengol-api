# âœ… Vertex AI Migration Setup - COMPLETE

**Status**: All code and infrastructure scripts ready for deployment
**Date**: November 8, 2025
**Migration**: d-vecDB VPS â†’ Google Vertex AI + Cloud Storage

---

## ğŸ¯ What Was Created

### 1. **Complete Infrastructure Scripts**

All scripts in `scripts/` directory:

- **`run-all-setup.sh`** â­ **MASTER SCRIPT** - Run this to set up everything
- `setup-vertex-ai-infrastructure.sh` - Creates bucket, service account, compute instance
- `deploy-crawler.sh` - Deploys crawler app to instance
- `update-vercel-env.sh` - Updates Vercel environment variables
- `setup-vertex-ai-grounding.sh` - Instructions for Vertex AI data store
- `README.md` - Comprehensive setup documentation

### 2. **Crawler Application**

Python scripts in `crawler/` directory:

- **`crawler.py`** - Multi-source incident scraper
  - Scrapes CISA KEV catalog
  - Scrapes NVD CVE database
  - Template for breach databases
  - Uploads to Cloud Storage

- **`embedding-pipeline.py`** - Incremental embedding generator
  - Processes raw incident files
  - Generates 768-dim embeddings using Vertex AI
  - Saves processed data with embeddings
  - Automatic incremental processing

### 3. **Updated Backend Code**

Modified files for Vertex AI integration:

- `src/lib/vertex-ai-client.ts` - Complete Vertex AI RAG client
  - Embedding generation (text-embedding-004)
  - Vector similarity search
  - Cloud Storage integration
  - Same interface as d-vecDB (zero breaking changes)

- `src/services/incident-search.ts` - Uses Vertex AI client
- `src/services/dynamic-question-generator.ts` - Updated references
- `src/routes/health.routes.ts` - Vertex AI health checks
- `.env` - Google Cloud configuration

### 4. **Documentation**

Complete guides in `docs/` directory:

- `VERTEX_AI_MIGRATION_GUIDE.md` - Comprehensive migration guide
- `CRAWLER_DEPLOYMENT_GUIDE.md` - Detailed crawler setup
- `VPS_DOWN_DIAGNOSTIC_NOV8.md` - Original issue diagnosis

---

## ğŸš€ Quick Start (One Command)

To set up the entire infrastructure, run:

```bash
./scripts/run-all-setup.sh
```

**This will:**
1. Create Cloud Storage bucket âœ“
2. Setup service account âœ“
3. Create Compute Engine instance âœ“
4. Deploy crawler application âœ“
5. Update Vercel environment variables âœ“
6. Provide Vertex AI grounding instructions âœ“

**Time Required:** ~20-30 minutes (mostly automated)

---

## ğŸ“‹ Step-by-Step Instructions

If you prefer manual control:

### Step 1: Setup Infrastructure (5-10 min)

```bash
./scripts/setup-vertex-ai-infrastructure.sh
```

**Creates:**
- Bucket: `gs://sengol-incidents`
- Service Account: `sengol-api@sengolvertexapi.iam.gserviceaccount.com`
- Instance: `sengol-crawler` (e2-micro, us-central1-a)
- Key Files: `sengol-api-key.json`, `sengol-api-key-base64.txt`

### Step 2: Deploy Crawlers (2-3 min)

```bash
# Wait 90 seconds for instance to initialize, then:
./scripts/deploy-crawler.sh
```

**Deploys:**
- `crawler.py` to `/opt/sengol-crawler/`
- `embedding-pipeline.py` to `/opt/sengol-crawler/`
- Systemd services: `sengol-crawler.service`, `sengol-embedding.service`
- Cron jobs: Daily at 2 AM (crawler), 3 AM (embeddings)

### Step 3: Update Vercel (2 min)

```bash
./scripts/update-vercel-env.sh
```

**Adds:**
- `GOOGLE_CLOUD_PROJECT=sengolvertexapi`
- `VERTEX_AI_LOCATION=us-central1`
- `GCS_BUCKET_NAME=sengol-incidents`
- `GOOGLE_APPLICATION_CREDENTIALS_JSON=<base64-key>`

**Removes:**
- `DVECDB_HOST`
- `DVECDB_PORT`
- `DVECDB_COLLECTION`

### Step 4: Vertex AI Grounding (Manual - 5 min + 30-60 min indexing)

```bash
./scripts/setup-vertex-ai-grounding.sh
```

Follow the instructions to create Vertex AI Search data store in Google Cloud Console.

### Step 5: Deploy to Vercel (5 min)

```bash
npm run build
vercel --prod
```

---

## ğŸ”„ Data Pipeline

The complete pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DAILY AUTOMATED PIPELINE                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  2:00 AM - Crawler Runs                                 â”‚
â”‚  â”œâ”€â†’ Scrapes CISA KEV, NVD, Breach DBs                 â”‚
â”‚  â””â”€â†’ Saves to: gs://sengol-incidents/incidents/raw/    â”‚
â”‚                                                          â”‚
â”‚  3:00 AM - Embedding Pipeline Runs                      â”‚
â”‚  â”œâ”€â†’ Reads unprocessed files from /raw/                â”‚
â”‚  â”œâ”€â†’ Generates embeddings (Vertex AI)                  â”‚
â”‚  â”œâ”€â†’ Saves to: /processed/ and /embeddings/            â”‚
â”‚  â””â”€â†’ Only processes new files (incremental)            â”‚
â”‚                                                          â”‚
â”‚  Vertex AI Search (Continuous)                          â”‚
â”‚  â”œâ”€â†’ Monitors: /embeddings/ folder                     â”‚
â”‚  â”œâ”€â†’ Indexes new files automatically                   â”‚
â”‚  â””â”€â†’ Ready for API queries                             â”‚
â”‚                                                          â”‚
â”‚  API Query Time                                         â”‚
â”‚  â”œâ”€â†’ L1 Cache: Local (1-5ms) - Check                  â”‚
â”‚  â”œâ”€â†’ L2 Cache: Redis (20-50ms) - Check                â”‚
â”‚  â”œâ”€â†’ L3: Vertex AI (100-3000ms) - Query               â”‚
â”‚  â””â”€â†’ Returns: Top-K similar incidents                 â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Monitoring

### Check Crawler Status

```bash
# SSH into instance
gcloud compute ssh sengol-crawler --zone=us-central1-a

# View crawler logs
sudo journalctl -u sengol-crawler.service -f

# View embedding pipeline logs
sudo journalctl -u sengol-embedding.service -f

# Check cron logs
tail -f /var/log/sengol-crawler.log
tail -f /var/log/sengol-embedding.log
```

### Check Cloud Storage

```bash
# List all incident files
gsutil ls -lh gs://sengol-incidents/incidents/**/*.jsonl

# Count total incidents
gsutil cat gs://sengol-incidents/incidents/raw/**/*.jsonl | wc -l

# Check latest embeddings
gsutil ls -lh gs://sengol-incidents/incidents/embeddings/ | tail -5
```

### Check API Health

```bash
# Local development
curl http://localhost:4000/health/detailed

# Production
curl https://api.sengol.ai/health/detailed

# Check Vertex AI status
curl https://api.sengol.ai/health/detailed | jq '.checks.vertexai'
```

---

## ğŸ“ Key Features

### 1. **Zero Frontend Changes**
All API endpoints remain identical:
- `/api/review/:id/generate-questions` - Same interface
- Response format unchanged
- Request format unchanged

### 2. **Automatic Incremental Processing**
- Crawler runs daily, adds new incidents
- Embedding pipeline processes only new files
- Vertex AI automatically indexes new data
- No manual intervention needed

### 3. **3-Tier Caching (Maintained)**
- **L1**: Local memory (1-5ms) - 60%+ hit rate
- **L2**: Redis (20-50ms) - 30%+ hit rate
- **L3**: Vertex AI (100-3000ms) - <10% miss rate

### 4. **Real-Time Incident Data**
Crawler sources:
- **CISA KEV**: Known Exploited Vulnerabilities (high priority)
- **NVD**: National Vulnerability Database (comprehensive)
- **Breach DBs**: Real-world incident data (customizable)

### 5. **Scalable Architecture**
- **Crawler**: e2-micro (free tier eligible)
- **Storage**: Pay per GB (~$0.20/mo for 10GB)
- **Embeddings**: Pay per request (~$1-2/mo)
- **Search**: Pay per query (~$5-10/mo)
- **Total**: ~$6-12/month (vs $20-50/mo VPS)

---

## ğŸ’° Cost Breakdown

| Component | Monthly Cost | Notes |
|-----------|-------------|-------|
| Compute Engine (e2-micro) | **$0** | Free tier (first instance) |
| Cloud Storage (10GB) | $0.20 | Standard storage |
| Vertex AI Embeddings | $1-2 | ~100K texts/month |
| Vertex AI Search | $5-10 | ~10K queries/month |
| Network Egress | $0.12 | ~1GB/month |
| **Total** | **$6-12** | **vs $20-50 VPS** |

**Savings**: 50-80% cost reduction + managed service benefits

---

## ğŸ”’ Security

### Service Account Key

The `sengol-api-key.json` file is **CRITICAL** - keep it secure:

```bash
# Set restrictive permissions
chmod 600 sengol-api-key.json

# Never commit to git (already in .gitignore)
git status # Should not show the key file

# Store in Vercel (encrypted)
vercel env add GOOGLE_APPLICATION_CREDENTIALS_JSON production
# Paste contents of sengol-api-key-base64.txt
```

### Rotate Keys Regularly

```bash
# Every 90 days, rotate keys:
gcloud iam service-accounts keys list \
  --iam-account=sengol-api@sengolvertexapi.iam.gserviceaccount.com

# Delete old key, create new one
# Update Vercel with new key
```

---

## ğŸ› Troubleshooting

### Issue: "gcloud: command not found"

```bash
# Install Google Cloud SDK
brew install google-cloud-sdk  # macOS
# or
curl https://sdk.cloud.google.com | bash  # Linux
```

### Issue: "Bucket not found"

```bash
# Create bucket manually
gsutil mb -p sengolvertexapi -l us-central1 gs://sengol-incidents
```

### Issue: Crawler not running

```bash
# SSH and check logs
gcloud compute ssh sengol-crawler --zone=us-central1-a
sudo systemctl status sengol-crawler.service
sudo journalctl -u sengol-crawler.service -n 50
```

### Issue: No embeddings generated

```bash
# Manually run embedding pipeline
gcloud compute ssh sengol-crawler --zone=us-central1-a
cd /opt/sengol-crawler
source venv/bin/activate
python3 embedding-pipeline.py
```

### Issue: Vertex AI returns no results

**Check:**
1. Bucket populated: `gsutil ls gs://sengol-incidents/incidents/embeddings/`
2. Crawler ran: Check logs
3. Embeddings processed: Check logs
4. Vertex AI indexed: Check Console (30-60 min indexing time)

---

## âœ… Verification Checklist

After running setup:

- [ ] Bucket exists: `gsutil ls gs://sengol-incidents`
- [ ] Instance running: `gcloud compute instances list`
- [ ] Crawler deployed: `gcloud compute ssh sengol-crawler --command='ls /opt/sengol-crawler'`
- [ ] Services active: `gcloud compute ssh sengol-crawler --command='sudo systemctl status sengol-*'`
- [ ] Vercel vars set: `vercel env ls`
- [ ] Build passes: `npm run build`
- [ ] Dev server works: `npm run dev` â†’ `curl http://localhost:4000/health`
- [ ] Incidents scraped: `gsutil ls gs://sengol-incidents/incidents/raw/` (after 1st run)
- [ ] Embeddings generated: `gsutil ls gs://sengol-incidents/incidents/embeddings/` (after 2nd run)

---

## ğŸ‰ Success Criteria

**Migration is complete when:**

1. âœ… Infrastructure created (bucket, service account, instance)
2. âœ… Crawler deployed and running
3. âœ… Incidents scraped to Cloud Storage
4. âœ… Embeddings generated by pipeline
5. âœ… Vercel environment variables updated
6. âœ… API deployed to production
7. âœ… Health check shows Vertex AI healthy
8. âœ… Question generation works end-to-end

---

## ğŸ“ Support

**Documentation:**
- Migration Guide: `docs/VERTEX_AI_MIGRATION_GUIDE.md`
- Crawler Guide: `docs/CRAWLER_DEPLOYMENT_GUIDE.md`
- Script Documentation: `scripts/README.md`

**Logs:**
- Vercel: `vercel logs`
- Crawler: `gcloud compute ssh sengol-crawler --command='sudo journalctl -u sengol-crawler.service -f'`
- Embeddings: `gcloud compute ssh sengol-crawler --command='sudo journalctl -u sengol-embedding.service -f'`

**Google Cloud Console:**
- Compute: https://console.cloud.google.com/compute/instances
- Storage: https://console.cloud.google.com/storage/browser/sengol-incidents
- Vertex AI: https://console.cloud.google.com/vertex-ai

---

## ğŸš€ Ready to Deploy!

Run the master setup script to get started:

```bash
./scripts/run-all-setup.sh
```

**Estimated Total Time:**
- Automated setup: ~20-30 minutes
- Manual Vertex AI setup: ~5 minutes
- Indexing wait time: ~30-60 minutes
- **Total: ~1-2 hours** (mostly waiting for automation)

**Then deploy to Vercel:**

```bash
npm run build
vercel --prod
```

**ğŸŠ Migration Complete!**

---

**Last Updated**: November 8, 2025
**Status**: âœ… All code and scripts ready for deployment
